{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#336699\">Introduction to Data Science - CAP 394 [<img src=\"./img/icone_cap394.svg\" alt=\"CAP394 - Introduction to Data Science\" style=\"height:120px;\" align=\"right\">](http://www.lac.inpe.br/~rafael.santos/cap394.html)</span>\n",
    "\n",
    "\n",
    "<hr style=\"border:2px solid #0077b9;\">\n",
    "\n",
    "   \n",
    "<h1> <p align=\"right\"> Exploratory Analysis of Meteorological Radar Dataset </p> </h1>\n",
    "   \n",
    "<p align='center'> Project for course  Introduction to Data Science - CAP 394</p>\n",
    "\n",
    "#### Instructors: \n",
    "* Dr. Gilberto Queiroz \n",
    "* Dr. Rafael Santos\n",
    "\n",
    "#### Student: \n",
    "* Helvecio Leal Neto\n",
    "\n",
    "\n",
    "\n",
    "## Schedule\n",
    "     - [x] Data reading\n",
    "     - [x] Data visualization\n",
    "     - [x] Preprocessing\n",
    "     - [x] Cluster detection algorithm\n",
    "     - [x] Relational Directions\n",
    "     - [x] Create Data Functions\n",
    "     - [x] Visualization Radar image\n",
    "     - [x] Statistics and Results\n",
    "     - [ ] Final analysis and project elaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index\n",
    "\n",
    "  [About](#about) <br></br>\n",
    "  [The Data](#the_data)<br></br>\n",
    "  [Reading Data](#reading_data)<br></br>\n",
    "  \n",
    " <hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='about'></a>\n",
    "## About\n",
    "\n",
    "<p align=\"justify\"> <br>\n",
    "This work is part of the suite of applications for the Introduction to Data Science (CAP-394) course offered by the National Institute for Space Research.\n",
    "\n",
    "The purpose of this paper is to perform an exploratory analysis of radar data, gathering raw data, processing it and understanding the dynamics of cloud location from its center of mass, as well as counting the number of clusters per period.\n",
    "At the end of the exploratory process it will be possible to estimate the behavior of the clouds and relate their geographical location as well as correlate average precipitation rates per hour.\n",
    "\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='the_data'></a>\n",
    "## The Data\n",
    "\n",
    "<p align=\"justify\"><br>\n",
    "The data analyzed in this example consists of NC (NetCDF4) files. Each file corresponds to Rain Rate measurements collected by the Amazon Protection System (SIPAM) radar during the GoAmazon experiment periods, starting from January 2014 to December 2015, such files contain data on 12-minute time intervals. The dataset provides rain data over 2.5km from S-Band Radar located in lat: -3.148556, lon: -59.992000. Data dimensionality consists of a two-dimensional (241x241) matrix containing rain rate values, the Radar Radius range is approximately 240 km, covering an area of 1,500 kmÂ² over the state of Amazonas (Brazil).\n",
    "<p align=\"justify\"><br>\n",
    "Overview:\n",
    "\n",
    "\"S-band radar volumes from Manaus, Brazil were shared by SIPAM for the time period beginning in January 2014 to December 2015 partially coinciding with the GOAmazon field campaign. The corrected radar reflectivity from each volume were interpolated to a fixed grid and the rainfall products were generated. A single Z-R relation (Z=174.8R^1.56) was created using 2014 wet-season impact disdrometer data and applied to the 2.5 km SIPAM Manaus S-Band CAPPI data to generate rain rates for each radar volume. This is version 2.0a of the dataset, which benefits from improved quality control procedures to remove non-meteorological data. \" Schumacher</p>\n",
    "\n",
    "\n",
    "https://www.arm.gov/research/campaigns/amf2014goamazon <br>\n",
    "\n",
    "* Courtney Schumacher\tS-band Radar - Rain Rates\n",
    "\n",
    "<img src=\"img/SIPAM.png\" align=\"center\" alt=\"Drawing\" style=\"width: 600px;\">\n",
    "\n",
    "\n",
    "[Figure 01: REDEMET - Radar Manaus](https://www.redemet.aer.mil.br/?i=produtos&p=radares-meteorologicos)\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions about the data\n",
    "\n",
    "1. How can I do the data acquisition?\n",
    "2. How can I open the data?\n",
    "3. What are the functions to view the data content?\n",
    "4. How to format data efficiently?\n",
    "5. Which precipitation rate intensity thresholds are interesting?\n",
    "6. What techniques for clustering can be applied to the dataset?\n",
    "7. How to convert values in Rain Rate Fall to DBz?\n",
    "8. What geographic applications can be associated with data?\n",
    "9. How to organize and process data efficiently?\n",
    "10. How to use a library to plot radar images?\n",
    "11. How many clusters per hour and month are in the dataset?\n",
    "12. What is the accumulated rain fall present in the data?\n",
    "13. What are the directional relationships of each cluster?\n",
    "\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reading_data'></a>\n",
    "## 1. Basic read process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reading the metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T17:58:21.735025Z",
     "start_time": "2019-09-04T17:58:21.390007Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T17:59:15.441985Z",
     "start_time": "2019-09-04T17:59:15.414282Z"
    }
   },
   "outputs": [],
   "source": [
    "### open files day 2014 / 01 / 03\n",
    "day = 20150101\n",
    "\n",
    "### path contains dataset\n",
    "path = 'data/radar/'\n",
    "\n",
    "#read first file to extract static variable values\n",
    "first_file = path+str(day)+'/'+str(os.listdir(path+str(day))[0])\n",
    "    \n",
    "try:\n",
    "    xds = xr.open_dataset(first_file)   \n",
    "except Exception as e:\n",
    "    print('File not found')\n",
    "xds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T17:59:16.175248Z",
     "start_time": "2019-09-04T17:59:16.165940Z"
    }
   },
   "outputs": [],
   "source": [
    "# settingup the variables\n",
    "rr = xds.rain_rate                  ### Matrix with preciptation values\n",
    "runit = xds.rain_rate.units         ### Unit of rain_rate mm/h\n",
    "rkm = xds.rain_rate.height_km       ### Unit of matrix dimensions km\n",
    "lon = xds.lon0.data                 ### Coordinate Longitude Matrix\n",
    "lat = xds.lat0.data                 ### Corrdinate Latitude Matrix\n",
    "x0 = xds.x0                         ### Matrix of points\n",
    "y0 = xds.y0\n",
    "\n",
    "print('Unit of Rain Rate: ',runit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T17:59:16.904719Z",
     "start_time": "2019-09-04T17:59:16.890418Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def readData(date):\n",
    "    \n",
    "    path = 'data/radar/'\n",
    "    dataset = []\n",
    "    interval = len(os.listdir(path+str(date)))    \n",
    "    \n",
    "    # Original grid dimensions\n",
    "    nx = 241\n",
    "    ny = 241\n",
    "\n",
    "    # Define container\n",
    "    frames = np.zeros( (interval, nx, ny ) )    \n",
    "    \n",
    "    for i in range(interval):\n",
    "        d = str(path)+str(date)+'/'\n",
    "        file = (sorted(os.listdir(path+str(date)))[i])\n",
    "        xds = xr.open_dataset(d+file)\n",
    "        rr = xds.rain_rate\n",
    "        frames[i] =  rr\n",
    "            \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The values loaded for frames correspond to the number of daily observations, and their respective matrices with rainfall rate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T17:59:19.277384Z",
     "start_time": "2019-09-04T17:59:18.654048Z"
    }
   },
   "outputs": [],
   "source": [
    "frames = readData(day)\n",
    "print('The frames from dataset:data (times,x,y) -> ',frames.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot simple example for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T17:59:26.504450Z",
     "start_time": "2019-09-04T17:59:25.897042Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "## Plot simple figure from dataset\n",
    "\n",
    "day = 20150101\n",
    "\n",
    "figtime = 96\n",
    "filename = sorted(os.listdir(path+str(day)))[figtime]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.title(filename)\n",
    "plt.scatter(lon,lat,frames[figtime])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Pre-Processing Function\n",
    "\n",
    "This function is used to store matrix \"x\" and \"y\" coordinates, that have values diffent than \"NaN\" and greater than the fixed Threshold for each cloud, the values correspond to the characteristic threshold intensity of each point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dbz_scale.png\" align=\"center\" alt=\"Drawing\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T17:59:29.937326Z",
     "start_time": "2019-09-04T17:59:29.934027Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def pre_processing(time1):\n",
    "    np.warnings.filterwarnings('ignore')\n",
    "    \n",
    "    ## thereshold value to track\n",
    "    threshold = 21.8\n",
    "    \n",
    "    rs =  (np.where(time1 != np.nan) and np.where(time1 > threshold))\n",
    "    rs = np.asanyarray(rs)\n",
    "    pe = pd.DataFrame({'x1':rs[0],'y1':rs[1]})\n",
    "        \n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T17:59:31.620965Z",
     "start_time": "2019-09-04T17:59:31.604562Z"
    }
   },
   "outputs": [],
   "source": [
    "pdata = pre_processing(frames[96])\n",
    "pdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre_processing function output with values \"x\" and \"y\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Clustering Process with MeanShift\n",
    "\n",
    "The purpose of the Mean-Shift clustering technique is to infer the average of clusters by their related density function, where an area calculation is made where the density values are higher and at this point a central point called centroid is measured. The process is performed recursively and is only terminated when the inference value is equal to the previous one.\n",
    "\n",
    "<img src=\"img/ms_2d_bw_2.gif\" align=\"left\" alt=\"Drawing\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:00:21.034705Z",
     "start_time": "2019-09-04T18:00:21.027092Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "def clust(time1):\n",
    "    \n",
    "    te = time1\n",
    "    \n",
    "    if len(te) < 20:\n",
    "        return None\n",
    "\n",
    "    bandwidth = estimate_bandwidth(te, quantile=0.3, n_samples=None, random_state=0, n_jobs=None)\n",
    "    \n",
    "    if bandwidth > 0:\n",
    "        ms = MeanShift(bandwidth=5, bin_seeding=None, cluster_all=True, min_bin_freq=1,\n",
    "    n_jobs=None, seeds=None)\n",
    "    else:\n",
    "        ms = MeanShift(bandwidth=10, bin_seeding=None, cluster_all=True, min_bin_freq=1,\n",
    "    n_jobs=None, seeds=None)\n",
    "\n",
    "    ms.fit(te)\n",
    "    labels = ms.labels_\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "    n_clusters_ = len(np.unique(labels))\n",
    "    te['cluster']=labels\n",
    "        \n",
    "#     colors = 10*['r.','g.','b.','c.','k.','y.','m.']\n",
    "#     for i in range(len(te)):\n",
    "#         plt.plot(te['x1'][i], te['y1'][i], colors[labels[i]], markersize = 10)\n",
    "#         plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "        \n",
    "#     for i, txt in enumerate(range(n_clusters_)):\n",
    "#         plt.annotate(txt,(cluster_centers[i,0],cluster_centers[i,1]),textcoords=\"offset points\",xytext=(0,10),ha='center')\n",
    "#     te['cluster']=labels\n",
    "    \n",
    "#     plt.scatter(cluster_centers[:,0], cluster_centers[:,1],\n",
    "#                marker = 'x', s=150, linewidths=10, zorder=10)\n",
    "    \n",
    "    return te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:00:02.232979Z",
     "start_time": "2019-09-04T18:00:01.096485Z"
    }
   },
   "outputs": [],
   "source": [
    "day = 20150101\n",
    "frames = readData(day)\n",
    "pdata = pre_processing(frames[90])\n",
    "clusters = clust(pdata)\n",
    "clusters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Direction Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define direction relationships between the central points of each cloud and the radar, we use the directional relationship between the points. This approach has as its principle to relate the coordinates of the points and to verify the location between a reference point and a point to be measured. According (Papadias & Sellis, 1994), there are nine possible ways to place a primary point relative to a reference point in the plane, and thus nine disjoint relationships between coordinate pairs that provide complete object coverage.\n",
    "\n",
    "<img src=\"img/direction_relations.png\" align=\"center\" alt=\"Drawing\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:00:07.344689Z",
     "start_time": "2019-09-04T18:00:07.325820Z"
    }
   },
   "outputs": [],
   "source": [
    "def tRelation(p,q):\n",
    "\n",
    "    north_west       =  p[1] <  q[1] and  p[0] >  q[0] \n",
    "    restricted_north =  p[1] == q[1] and  p[0] >  q[0]\n",
    "    north_east       =  p[1] >  q[1] and  p[0] >  q[0]\n",
    "    restricted_west  =  p[1] <  q[1] and  p[0] == q[0]\n",
    "    same_position    =  p[1] == q[1] and  p[0] == q[0]\n",
    "    restricted_east  =  p[1] >  q[1] and  p[0] == q[0]\n",
    "    south_west       =  p[1] <  q[1] and  p[0] <  q[0]\n",
    "    restricted_south =  p[1] == q[1] and  p[0] <  q[0] \n",
    "    south_est        =  p[1] >  q[1] and  p[0] <  q[0]\n",
    "\n",
    "    if north_west == True:\n",
    "        return ('NW')\n",
    "    if restricted_north == True:\n",
    "        return ('RN')\n",
    "    if north_east == True:\n",
    "        return ('NE')\n",
    "    if restricted_west == True:\n",
    "        return ('RW')\n",
    "    if same_position == True:\n",
    "        return('SP')\n",
    "    if restricted_east == True:\n",
    "        return ('RE')\n",
    "    if south_west == True:\n",
    "        return ('SW')\n",
    "    if restricted_south == True:\n",
    "        return ('RS')\n",
    "    if south_est == True:\n",
    "        return ('SE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:00:07.614037Z",
     "start_time": "2019-09-04T18:00:07.605061Z"
    }
   },
   "outputs": [],
   "source": [
    "x1,y1 = -1.9451030492782593,-61.034568786621094      # p(POINT)\n",
    "r1,r2 = -3.148556, -59.992000                        # q(RADAR)\n",
    "p = (x1,y1)                                    #PONTO\n",
    "q = (r1,r2)                                    #RADAR\n",
    "\n",
    "print('Point p is ',tRelation(p,q),' of the radar.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Create Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:00:10.128922Z",
     "start_time": "2019-09-04T18:00:10.110067Z"
    }
   },
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "def createData(day,time,clusters,frames):\n",
    "    \n",
    "    ##Static Radar Coordinates Value for Topological relation\n",
    "    radar = (-3.148556, -59.992000)\n",
    "\n",
    "    if isinstance(clusters,pd.DataFrame):\n",
    "\n",
    "        FAM1 = pd.DataFrame(columns=['DATETIME',\n",
    "                                     'N_Cluster','ID_CLUS','LAT','LON','DIST','IND_X','IND_Y',\n",
    "                                     'T_RELATION','RAIN_FALL','DBz'])\n",
    "        \n",
    "        rfall = []\n",
    "        rlation = []\n",
    "        ddist = []\n",
    "\n",
    "        LAT_ = (lat[clusters['x1'],clusters['y1']])\n",
    "        LON_ = (lon[clusters['x1'],clusters['y1']])\n",
    "        \n",
    "        # Calculate distance\n",
    "        for i, row in clusters.iterrows():\n",
    "            pt =(str(LAT_[i])+','+str(LON_[i]))\n",
    "            d = (geodesic(radar, pt).kilometers)\n",
    "            ddist.append(d)\n",
    "\n",
    "        # Calculate relation\n",
    "        for i,row in clusters.iterrows():\n",
    "            rfall.append(frames[row['x1']][row['y1']])\n",
    "            r = tRelation((LAT_[i],LON_[i]),radar)\n",
    "            rlation.append(r)\n",
    "            \n",
    "        xds = xr.open_dataset(str(path)+str(day)+'/'+str(sorted(os.listdir(path+str(day)))[time]))\n",
    "        dtime = xds.start_time.values\n",
    "        \n",
    "        FAM1['IND_X'], FAM1['IND_Y'] = clusters['x1'].astype('int16'),clusters['y1'].astype('int16')\n",
    "        FAM1['LAT'],FAM1['LON'] = LAT_,LON_\n",
    "        FAM1['DIST'] = ddist\n",
    "        FAM1['N_Cluster'] = len(clusters['cluster'].unique())\n",
    "        FAM1['ID_CLUS'] = clusters['cluster']\n",
    "        FAM1['RAIN_FALL'] = rfall\n",
    "        FAM1['T_RELATION'] = rlation\n",
    "        FAM1['DBz'] =  10 * np.log10(200*FAM1['RAIN_FALL']**1.6)\n",
    "        FAM1['DATETIME'] = pd.to_datetime(dtime)\n",
    "        FAM1 = FAM1.set_index('DATETIME')\n",
    "        FAM1 = FAM1.sort_values('ID_CLUS')\n",
    "        \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    return FAM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:00:14.172553Z",
     "start_time": "2019-09-04T18:00:13.060964Z"
    }
   },
   "outputs": [],
   "source": [
    "day = 20150101\n",
    "time = 96\n",
    "frames = readData(day)\n",
    "pdata = pre_processing(frames[time])\n",
    "clusters = clust(pdata)\n",
    "cdata = createData(day,time,clusters,frames[time])\n",
    "cdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Extract Centroid by Max RAIN_FALL value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:00:27.231649Z",
     "start_time": "2019-09-04T18:00:27.227623Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def centroidData(clus):\n",
    "    if isinstance(clus,pd.DataFrame):\n",
    "        centroid = pd.DataFrame()\n",
    "            \n",
    "        for i in range(clus['N_Cluster'].max()):\n",
    "            ct = clus.loc[clus['ID_CLUS'] == i ]\n",
    "            ct = ct.loc[ct['RAIN_FALL'] == ct['RAIN_FALL'].max()]\n",
    "            centroid = centroid.append(ct)\n",
    "    else:\n",
    "        return None\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:00:32.745452Z",
     "start_time": "2019-09-04T18:00:32.670196Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = centroidData(cdata)\n",
    "ct.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:00:36.685517Z",
     "start_time": "2019-09-04T18:00:36.678257Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(day):\n",
    "    data = readData(day)\n",
    "    cent = pd.DataFrame()\n",
    "    dados = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        proc = pre_processing(data[i]) \n",
    "        cluster = clust(proc)\n",
    "             \n",
    "        if isinstance(cluster,pd.DataFrame):\n",
    "            clt = createData(day,i,cluster,data[i])\n",
    "            dados = dados.append(clt)\n",
    "            clt2 = centroidData(clt)\n",
    "            cent = cent.append(clt2)\n",
    "        else:\n",
    "            pass\n",
    "    dados.to_csv('output/clusters/fam_'+str(sorted(os.listdir(path+str(day)))[i][:-3])+'.csv')\n",
    "    cent.to_csv('output/centroids/centroid_'+str(sorted(os.listdir(path+str(day)))[i][:-3])+'.csv')\n",
    "    return cent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.9 Run process per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T14:25:44.130119Z",
     "start_time": "2019-09-04T14:25:41.347664Z"
    }
   },
   "outputs": [],
   "source": [
    "day = 20140928\n",
    "runner = run(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:27:37.692162Z",
     "start_time": "2019-09-04T18:00:54.945979Z"
    }
   },
   "outputs": [],
   "source": [
    "path = 'data/radar/'\n",
    "\n",
    "for day in sorted(os.listdir(path)):\n",
    "    rodada = run(day)\n",
    "    rodada = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9.1 Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:27:50.571609Z",
     "start_time": "2019-09-04T18:27:48.094112Z"
    }
   },
   "outputs": [],
   "source": [
    "day = 20150101\n",
    "plot(96,day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:27:45.417968Z",
     "start_time": "2019-09-04T18:27:45.282949Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib import animation\n",
    "from matplotlib import markers\n",
    "\n",
    "def plot(frame,day):\n",
    "\n",
    "    data = readData(day)\n",
    "    proc = pre_processing(data[frame])\n",
    "    clt = clust(proc)\n",
    "\n",
    "    cdata = createData(day,frame,clt,data[frame])\n",
    "#     centers = centroid_(day,frame,clt,data[frame])\n",
    "    centers = centroidData(cdata)\n",
    "\n",
    "    if centers is not None:\n",
    "        c_lat = centers['LAT']\n",
    "        c_lon = centers['LON']\n",
    "        \n",
    "        centroid = []\n",
    "        for i in range(centers['N_Cluster'].max()):\n",
    "            ct = centers.loc[centers['ID_CLUS'] == i ]\n",
    "#             ct = ct.loc[ct['RAIN_FALL'] == ct['RAIN_FALL'].max()]\n",
    "            centroid.append(ct) \n",
    "    else:\n",
    "        centroid = ''\n",
    "        \n",
    "    file = path+str(day)+'/'+str(sorted(os.listdir(path+str(day)))[frame])\n",
    "    xds = xr.open_dataset(file)\n",
    "    date_time = xds.start_time.data\n",
    "    date_time = pd.to_datetime(date_time)\n",
    "\n",
    "    my_coords = [-3.148556, -59.992000]     ## RADAR T1 SIPAM COORDS\n",
    "    zoom_scale = 2.2                        ## ZOOM SCALE\n",
    "\n",
    "    bbox = [my_coords[0]-zoom_scale,my_coords[0]+zoom_scale,\\\n",
    "             my_coords[1]-zoom_scale,my_coords[1]+zoom_scale]\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1,ncols=1,figsize=(10,10),dpi=100)\n",
    "    label = 'Rain rate in ' + runit+ ''\n",
    "    title = 'SIPAM Manaus S-Band Radar :' + str(date_time)\n",
    "\n",
    "    # draw filled contours.\n",
    "    clevs = [0, 1, 2.5, 5, 7.5, 10, 15, 20, 30, 40,\n",
    "                 50, 70, 100]\n",
    "\n",
    "    cmap_data = [(1.0, 1.0, 1.0),\n",
    "                     (0.3137255012989044, 0.8156862854957581, 0.8156862854957581),\n",
    "                     (0.0, 1.0, 1.0),\n",
    "                     (0.0, 0.8784313797950745, 0.501960813999176),\n",
    "                     (0.0, 0.7529411911964417, 0.0),\n",
    "                     (0.501960813999176, 0.8784313797950745, 0.0),\n",
    "                     (1.0, 1.0, 0.0),\n",
    "                     (1.0, 0.6274510025978088, 0.0),\n",
    "                     (1.0, 0.0, 0.0),\n",
    "                     (1.0, 0.125490203499794, 0.501960813999176),\n",
    "                     (0.9411764740943909, 0.250980406999588, 1.0),\n",
    "                     (0.501960813999176, 0.125490203499794, 1.0),\n",
    "                     (0.250980406999588, 0.250980406999588, 1.0)]\n",
    "\n",
    "    cmap = mcolors.ListedColormap(cmap_data, 'precipitation')\n",
    "    norm = mcolors.BoundaryNorm(clevs, cmap.N)\n",
    "    ax = axes\n",
    "\n",
    "    m = Basemap(projection='merc',llcrnrlat=bbox[0],urcrnrlat=bbox[1],\\\n",
    "                    llcrnrlon=bbox[2],urcrnrlon=bbox[3],lat_ts=10,resolution='i')\n",
    "\n",
    "    ## PRECIPTACAO\n",
    "    xi, yi = m(lon, lat)\n",
    "    ## SIPAM RADAR\n",
    "    xm, ym = m(my_coords[1],my_coords[0])\n",
    "    radar = m.plot(xm,ym, marker='^',color='r', label='RADAR')\n",
    "\n",
    "    for cent in range(len(centroid)):\n",
    "        clat, clon, mm_f = centroid[cent]['LAT'].item(),centroid[cent]['LON'].item(),centroid[cent]['RAIN_FALL'].item()\n",
    "        #clat, clon, mm_f = centroid[1]['LAT'].item(),centroid[1]['LON'].item(),centroid[1]['RAIN_FALL'].item()\n",
    "        t3x,t3y = m(clon, clat)\n",
    "        m.plot(t3x,t3y, marker=markers.CARETDOWN, markersize=10, color='k')\n",
    "#         plt.annotate(str(mm_f)[0:5]+'mm/h', xy=(t3x,t3y),xytext=(t3x+12,t3y+12),rotation=45, size=10)\n",
    "\n",
    "    m.plot(xm,ym, label='NÂº Clusters: ' +str(len(centroid)),marker=markers.CARETDOWN, color='k')\n",
    "\n",
    "    cs = m.pcolormesh(xi,yi,data[frame], cmap = cmap, norm = norm, ax=ax)\n",
    "\n",
    "    # # # # Add Grid Lines\n",
    "    m.drawparallels(np.arange(bbox[0],bbox[1],(bbox[1]-bbox[0])/5),labels=[1,0,0,0],rotation=45, size=(7))\n",
    "    m.drawmeridians(np.arange(bbox[2],bbox[3],(bbox[3]-bbox[2])/5),labels=[0,0,0,1],rotation=45, size=(7))\n",
    "    m.drawmapboundary(fill_color='gray')\n",
    "\n",
    "    m.readshapefile('./img/hidro/lineaire_1km', 'hidro10km', color='b', linewidth=1)\n",
    "    # # # # Add Colorbar\n",
    "    cbar = m.colorbar(cs, location='bottom', pad=\"10%\")\n",
    "    cbar.set_label(label)\n",
    "\n",
    "    # # # # # Add Title\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.ylabel('Longitude', labelpad=40)\n",
    "    plt.xlabel('Latitude', labelpad=60)\n",
    "\n",
    "    plt.savefig('radar_image/'+ sorted(os.listdir(path+str(day)))[frame]+'.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-04T14:19:16.794Z"
    }
   },
   "outputs": [],
   "source": [
    "## CREATE ALL FIGS\n",
    "for i in range(len(frames)):\n",
    "    plot(i,day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:28:04.350800Z",
     "start_time": "2019-09-04T18:28:04.347977Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:28:14.387502Z",
     "start_time": "2019-09-04T18:28:04.836617Z"
    }
   },
   "outputs": [],
   "source": [
    "l = [pd.read_csv(filename) for filename in sorted(glob.glob(\"output/centroids/*.csv\"))]\n",
    "dados = pd.concat(l, axis=0, sort=False)\n",
    "dados['DATETIME'] =  pd.to_datetime(dados['DATETIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "dados = dados.set_index('DATETIME')\n",
    "dados.index = dados.index.map(lambda x: x.replace(second=0))\n",
    "\n",
    "freqtime = pd.date_range(start=dados.index.min(), end=dados.index.max(), freq='12T')\n",
    "\n",
    "time = []\n",
    "\n",
    "for i in range(len(freqtime)):\n",
    "    t = dados.loc[dados.index == freqtime[i]]\n",
    "    time.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:28:16.612433Z",
     "start_time": "2019-09-04T18:28:16.598048Z"
    }
   },
   "outputs": [],
   "source": [
    "def calcDist(t1,t2):\n",
    "    \n",
    "    if len(t1) == 0 or len(t2) == 0:\n",
    "        return None\n",
    "    \n",
    "    time1 = t1\n",
    "    time2 = t2\n",
    "\n",
    "    t1 = t1[['ID_CLUS','IND_X','IND_Y']]\n",
    "    t1_data = t1.values.tolist()\n",
    "\n",
    "    t2 = t2[['ID_CLUS','IND_X','IND_Y']]\n",
    "    t2_data = t2.values.tolist()\n",
    "\n",
    "    t1_ = pd.DataFrame(data=t1_data, columns=['T1', 'X', 'Y'])\n",
    "    t2_ = pd.DataFrame(data=t2_data, columns=['T2', 'X', 'Y'])\n",
    "\n",
    "    mask = cdist(t2_[['X', 'Y']].values, t1_[['X', 'Y']].values,'euclidean') < 5\n",
    "\n",
    "    arr = []\n",
    "    def zone(x):\n",
    "        arr.append ( t1_[x].T1.values[0] if x.any() else np.nan)\n",
    "        \n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    np.apply_along_axis(zone, True, mask)\n",
    "    result['T1'] = arr\n",
    "    result['T2'] = t2_.T2.values\n",
    "\n",
    "    \n",
    "    result['START_TIME'] = t1.index[0]\n",
    "    result['END_TIME'] = t2.index[0]\n",
    "    result['X2'] = (t2.IND_X.values)\n",
    "    result['Y2'] = (t2.IND_Y.values)\n",
    "    result['LAT2'] = (time2.LAT.values)\n",
    "    result['LON2'] = (time2.LON.values)\n",
    "\n",
    "    x1,y1 = [],[]\n",
    "    lat1,lon1 = [],[]\n",
    "    lat2,lon2 = [],[]\n",
    "    dist = []\n",
    "\n",
    "    for i in range(len(arr)):\n",
    "        a = time1.loc[time1.ID_CLUS == arr[i]]\n",
    "        b = time2.loc[time2.ID_CLUS == arr[i]]\n",
    "        x1.append(a['IND_X'].values)\n",
    "        y1.append(a['IND_Y'].values)\n",
    "\n",
    "        lat1.append(a['LAT'].values)\n",
    "        lon1.append(a['LON'].values)\n",
    "\n",
    "    result['X1']= x1\n",
    "    result['Y1']= y1\n",
    "    result['LAT1'] = lat1\n",
    "    result['LON1'] = lon1\n",
    "\n",
    "    result['X1'] = result['X1'].str[0]\n",
    "    result['Y1'] = result['Y1'].str[0]\n",
    "    result['LAT1'] = result['LAT1'].str[0]\n",
    "    result['LON1'] = result['LON1'].str[0]\n",
    "    \n",
    "    result = result.sort_values('T1')\n",
    "    result = result.dropna()\n",
    "    result['T1'] = result['T1'].astype(int)\n",
    "    result['X1'] = result['X1'].astype(int)\n",
    "    result['Y1'] = result['Y1'].astype(int)\n",
    "    \n",
    "    dist,vel,rel = [],[],[]\n",
    "    \n",
    "    ## CALC DIST\n",
    "    for i,row in result.iterrows():\n",
    "        p1 = (str(row.LAT1)+','+str(row.LON1))\n",
    "        p2 = (str(row.LAT2)+','+str(row.LON2))\n",
    "        dist.append(geodesic(p1, p2).kilometers)\n",
    "        d1 = (row.LAT1,row.LON1)\n",
    "        d2 = (row.LAT2,row.LON2)\n",
    "        rel.append(tRelation(d1,d2))\n",
    "    \n",
    "    result['DIST'] = dist\n",
    "    result['REL'] = rel\n",
    "\n",
    "    ## CALV VEL\n",
    "    for i,row in result.iterrows():\n",
    "        vel.append(row.DIST/0.2)\n",
    "    result['VELM'] = vel\n",
    "    result = result[['START_TIME','END_TIME','T1','T2','X1','Y1','X2','Y2','LAT1','LON1','LAT2','LON2','DIST','VELM','REL']]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:36:29.594954Z",
     "start_time": "2019-09-04T18:28:20.718164Z"
    }
   },
   "outputs": [],
   "source": [
    "track = pd.DataFrame()\n",
    "\n",
    "aux = []\n",
    "\n",
    "for i in range(len(time)):\n",
    "    if len(time[i]) > 0:\n",
    "        aux.append(i)\n",
    "for i in range(len(aux)-1):\n",
    "    track = track.append(calcDist(time[aux[i]],time[aux[i+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:36:33.271480Z",
     "start_time": "2019-09-04T18:36:32.892262Z"
    }
   },
   "outputs": [],
   "source": [
    "track.head()\n",
    "track.to_csv('output/tracking/tracking.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Statistics and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:37:22.230968Z",
     "start_time": "2019-09-04T18:37:21.942069Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:40:28.942467Z",
     "start_time": "2019-09-04T18:40:28.287991Z"
    }
   },
   "outputs": [],
   "source": [
    "l = [pd.read_csv(filename) for filename in sorted(glob.glob(\"output/centroids/*.csv\"))]\n",
    "dados = pd.concat(l, axis=0, sort=False)\n",
    "dados = dados.loc[:, ~dados.columns.str.contains('^Unnamed')]\n",
    "dados['DATETIME'] =  pd.to_datetime(dados['DATETIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "dados = dados.set_index('DATETIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:40:30.247174Z",
     "start_time": "2019-09-04T18:40:30.234273Z"
    }
   },
   "outputs": [],
   "source": [
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:40:44.943844Z",
     "start_time": "2019-09-04T18:40:44.941501Z"
    }
   },
   "outputs": [],
   "source": [
    "tidy_data2015 = pd.DataFrame()\n",
    "tidy_data2015 = dados\n",
    "\n",
    "# data2014 = dados.loc[dados['YEAR'] == 2014]\n",
    "# data2015 = dados.loc[dados['YEAR'] == 2015]\n",
    "\n",
    "# tidy_data2014 = pd.DataFrame()\n",
    "# tidy_data2015 = pd.DataFrame()\n",
    "# tidy_data2015 = dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:40:51.681616Z",
     "start_time": "2019-09-04T18:40:51.663999Z"
    }
   },
   "outputs": [],
   "source": [
    "tidy_data2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:39:53.710278Z",
     "start_time": "2019-08-30T14:39:36.916538Z"
    }
   },
   "outputs": [],
   "source": [
    "## FOR 2014\n",
    "data2014 = data2014[['YEAR','MONTH','DAY','HOUR','MINUTE','N_Cluster','ID_CLUS','T_RELATION','RAIN_FALL','DBz']]\n",
    "\n",
    "data2014['YEAR'] = data2014['YEAR'].astype(int)\n",
    "data2014['MONTH'] = data2014['MONTH'].astype(int)\n",
    "data2014['DAY'] = data2014['DAY'].astype(int)\n",
    "data2014['HOUR'] = data2014['HOUR'].astype(int)\n",
    "data2014['MINUTE'] = data2014['MINUTE'].astype(int)\n",
    "data2014['N_Cluster'] = data2014['N_Cluster'].astype(int)\n",
    "data2014['ID_CLUS'] = data2014['ID_CLUS'].astype(int)\n",
    "data2014['T_RELATION'] = data2014['T_RELATION'].astype(str)\n",
    "\n",
    "data2014['DateTime'] = data2014.apply(lambda row: datetime(\n",
    "                              row['YEAR'], row['MONTH'], row['DAY'], row['HOUR'], row['MINUTE']), axis=1)\n",
    "\n",
    "data2014 = data2014.drop(columns=['YEAR','MONTH','DAY','HOUR','MINUTE'])\n",
    "data2014 = data2014.set_index('DateTime')\n",
    "\n",
    "tidy_data2014 = data2014\n",
    "\n",
    "## FOR 2015\n",
    "data2015 = data2015[['YEAR','MONTH','DAY','HOUR','MINUTE','N_Cluster','ID_CLUS','T_RELATION','RAIN_FALL','DBz']]\n",
    "\n",
    "data2015['YEAR'] = data2015['YEAR'].astype(int)\n",
    "data2015['MONTH'] = data2015['MONTH'].astype(int)\n",
    "data2015['DAY'] = data2015['DAY'].astype(int)\n",
    "data2015['HOUR'] = data2015['HOUR'].astype(int)\n",
    "data2015['MINUTE'] = data2015['MINUTE'].astype(int)\n",
    "data2015['N_Cluster'] = data2015['N_Cluster'].astype(int)\n",
    "data2015['ID_CLUS'] = data2015['ID_CLUS'].astype(int)\n",
    "data2015['T_RELATION'] = data2015['T_RELATION'].astype(str)\n",
    "\n",
    "data2015['DateTime'] = data2015.apply(lambda row: datetime(\n",
    "                              row['YEAR'], row['MONTH'], row['DAY'], row['HOUR'], row['MINUTE']), axis=1)\n",
    "\n",
    "data2015 = data2015.drop(columns=['YEAR','MONTH','DAY','HOUR','MINUTE'])\n",
    "data2015 = data2015.set_index('DateTime')\n",
    "\n",
    "tidy_data2015 = data2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:39:53.727193Z",
     "start_time": "2019-08-30T14:39:53.712430Z"
    }
   },
   "outputs": [],
   "source": [
    "tidy_data2014.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:39:53.829948Z",
     "start_time": "2019-08-30T14:39:53.729002Z"
    }
   },
   "outputs": [],
   "source": [
    "tidy_data2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:41:39.672939Z",
     "start_time": "2019-09-04T18:41:39.602920Z"
    }
   },
   "outputs": [],
   "source": [
    "# NCluster = tidy_data2014.resample('min').mean()\n",
    "# NCluster = NCluster.dropna()\n",
    "# NCluster = NCluster.resample('M')[['N_Cluster']].sum()\n",
    "\n",
    "NCluster5 = tidy_data2015.resample('min').mean()\n",
    "NCluster5 = NCluster5.dropna()\n",
    "NCluster5 = NCluster5.resample('M')[['N_Cluster']].sum()\n",
    "\n",
    "# cdf2014 = pd.DataFrame({'date':NCluster.index, 'clusters':NCluster.values.ravel()})\n",
    "cdf2015 = pd.DataFrame({'date':NCluster5.index, 'clusters':NCluster5.values.ravel()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:39:54.716750Z",
     "start_time": "2019-08-30T14:39:54.134081Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=cdf2014.date, y=cdf2014['clusters'], name=\"2014\",\n",
    "                         line_color='deepskyblue'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=cdf2014.date, y=cdf2015['clusters'], name=\"2015\",\n",
    "                         line_color='dimgray'))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=go.layout.Title(\n",
    "        text=\"OcorrÃªncias de Clusters para os anos de 2014 e 2015\",\n",
    "        xref=\"paper\",\n",
    "        xanchor = \"auto\",\n",
    "        x=0,\n",
    "    ),\n",
    "    yaxis=go.layout.YAxis(\n",
    "        title=go.layout.yaxis.Title(\n",
    "            text=\"NÃºmero de clusters\",\n",
    "            font=dict(\n",
    "                family=\"Courier New, monospace\",\n",
    "                size=14,\n",
    "                color=\"#7f7f7f\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:41:43.135552Z",
     "start_time": "2019-09-04T18:41:43.123205Z"
    }
   },
   "outputs": [],
   "source": [
    "NRain = tidy_data2014.resample('min').mean()\n",
    "NRain = NRain.dropna()\n",
    "NRain = NRain.resample('M')[['RAIN_FALL']].mean()\n",
    "\n",
    "NRain5 = tidy_data2015.resample('min').mean()\n",
    "NRain5 = NRain5.dropna()\n",
    "NRain5 = NRain5.resample('M')[['RAIN_FALL']].mean()\n",
    "\n",
    "rdf2014 = pd.DataFrame({'date':NRain.index, 'clusters':NRain.values.ravel()})\n",
    "rdf2015 = pd.DataFrame({'date':NRain5.index, 'clusters':NRain5.values.ravel()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:39:55.013799Z",
     "start_time": "2019-08-30T14:39:54.885186Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=rdf2014.date, y=rdf2014['clusters'], name=\"2014\",\n",
    "                         line_color='deepskyblue'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=rdf2014.date, y=rdf2015['clusters'], name=\"2015\",\n",
    "                         line_color='dimgray'))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=go.layout.Title(\n",
    "        text=\"PrecipitaÃ§Ã£o mÃ©dia mensal de 2014 e 2015\",\n",
    "        xref=\"paper\",\n",
    "        xanchor = \"auto\",\n",
    "        x=0,\n",
    "    ),\n",
    "    yaxis=go.layout.YAxis(\n",
    "        title=go.layout.yaxis.Title(\n",
    "            text=\"PrecipitaÃ§Ã£o mÃ©dia por clusters\",\n",
    "            font=dict(\n",
    "                family=\"Courier New, monospace\",\n",
    "                size=14,\n",
    "                color=\"#7f7f7f\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:39:55.410329Z",
     "start_time": "2019-08-30T14:39:55.015942Z"
    }
   },
   "outputs": [],
   "source": [
    "an2014 = tidy_data2014.resample('Y')['T_RELATION'].value_counts()\n",
    "\n",
    "df2014 = pd.DataFrame({'RelaÃ§Ã£o':an2014.index.get_level_values('T_RELATION').values, 'Valores':an2014.values.ravel()})\n",
    "\n",
    "fig = px.bar(df2014, x='RelaÃ§Ã£o', y='Valores', color='RelaÃ§Ã£o', labels='RelaÃ§Ã£o')\n",
    "fig.update_layout(title_text='RelaÃ§Ãµes direcionais para o ano de 2014')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:39:55.702732Z",
     "start_time": "2019-08-30T14:39:55.412360Z"
    }
   },
   "outputs": [],
   "source": [
    "an2015 = tidy_data2015.resample('Y')['T_RELATION'].value_counts()\n",
    "\n",
    "an2015 = pd.DataFrame({'RelaÃ§Ã£o':an2015.index.get_level_values('T_RELATION').values, 'Valores':an2015.values.ravel()})\n",
    "\n",
    "fig = px.bar(an2015, x='RelaÃ§Ã£o', y='Valores', color='RelaÃ§Ã£o', labels='RelaÃ§Ã£o')\n",
    "fig.update_layout(title_text='RelaÃ§Ãµes direcionais para o ano de 2015')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:39:55.847775Z",
     "start_time": "2019-08-30T14:39:55.704331Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df2014['RelaÃ§Ã£o'],\n",
    "    y=df2014['Valores'],\n",
    "    name='2014',\n",
    "    marker_color='indianred'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    x=an2015['RelaÃ§Ã£o'],\n",
    "    y=an2015['Valores'],\n",
    "    name='2015',\n",
    "    marker_color='lightsalmon'\n",
    "))\n",
    "\n",
    "fig.update_layout(title_text='RelaÃ§Ãµes direcionais para o ano de 2015', barmode='group')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:39:56.342122Z",
     "start_time": "2019-08-30T14:39:55.850146Z"
    }
   },
   "outputs": [],
   "source": [
    "men2014 = tidy_data2014.resample('M')['T_RELATION'].value_counts()\n",
    "\n",
    "m2014 = pd.DataFrame({'Data': men2014.index.get_level_values('DateTime').values.ravel(),\n",
    "                      'RelaÃ§Ã£o':men2014.index.get_level_values('T_RELATION').values.ravel(),\n",
    "                      'Valores':men2014.values.ravel()})\n",
    "\n",
    "m2014['Data'] = pd.to_datetime(m2014['Data'])\n",
    "\n",
    "fig = px.bar(m2014, x='Data', y='Valores', color='RelaÃ§Ã£o', labels='RelaÃ§Ã£o',barmode='group')\n",
    "fig.update_layout(title_text='RelaÃ§Ãµes direcionais para o ano de 2014')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T18:41:11.507422Z",
     "start_time": "2019-09-04T18:41:11.473340Z"
    }
   },
   "outputs": [],
   "source": [
    "men2015 = tidy_data2015.resample('M')['T_RELATION'].value_counts()\n",
    "\n",
    "m2015 = pd.DataFrame({'Data': men2015.index.get_level_values('DateTime').values.ravel(),\n",
    "                      'RelaÃ§Ã£o':men2015.index.get_level_values('T_RELATION').values.ravel(),\n",
    "                      'Valores':men2015.values.ravel()})\n",
    "\n",
    "m2015['Data'] = pd.to_datetime(m2015['Data'])\n",
    "\n",
    "fig = px.bar(m2015, x='Data', y='Valores', color='RelaÃ§Ã£o', labels='RelaÃ§Ã£o',barmode='group')\n",
    "fig.update_layout(title_text='RelaÃ§Ãµes direcionais para o ano de 2015')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORTRACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:39:56.717749Z",
     "start_time": "2019-08-30T14:39:56.715323Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:41:09.339181Z",
     "start_time": "2019-08-30T14:39:56.720314Z"
    }
   },
   "outputs": [],
   "source": [
    "f = [pd.read_csv(filename, delimiter=r\"\\s+\") for filename in sorted(glob.glob(\"output/fortracc/*.txt\"))]\n",
    "fortracc = pd.concat(f, axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:41:09.361425Z",
     "start_time": "2019-08-30T14:41:09.340516Z"
    }
   },
   "outputs": [],
   "source": [
    "fortracc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:48:36.008219Z",
     "start_time": "2019-08-30T14:41:09.363144Z"
    }
   },
   "outputs": [],
   "source": [
    "## FOR 2015\n",
    "fdata = pd.DataFrame(columns=['YEAR','MONTH','DAY','HOUR','MINUTE','N_Cluster','REFLECT','PRECIPIT'])\n",
    "\n",
    "fdata['YEAR'] = fortracc['YEAR'].astype('int16')\n",
    "fdata['MONTH'] = fortracc['MONTH'].astype('int16')\n",
    "fdata['DAY'] = fortracc['DAY'].astype('int16')\n",
    "fdata['HOUR'] = fortracc['HOUR'].astype('int16')\n",
    "fdata['MINUTE'] = fortracc['MINUTE'].astype('int16')\n",
    "fdata['N_Cluster'] = fortracc['N_Cluster'].astype('int16')\n",
    "fdata['REFLECT'] = fortracc['REFLECT'].astype('float32')\n",
    "fdata['PRECIPIT'] = fortracc['PRECIPIT'].astype('float32')\n",
    "\n",
    "fdata['DATETIME'] = fdata.apply(lambda row: datetime(\n",
    "                              row['YEAR'], row['MONTH'], row['DAY'], row['HOUR'], row['MINUTE']), axis=1)\n",
    "\n",
    "fdata = fdata.drop(columns=['YEAR','MONTH','DAY','HOUR','MINUTE'])\n",
    "fdata = fdata.set_index('DATETIME')\n",
    "\n",
    "tidy_fortracc = fdata.sort_values(by='DATETIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:48:36.015762Z",
     "start_time": "2019-08-30T14:48:36.009376Z"
    }
   },
   "outputs": [],
   "source": [
    "tidy_fortracc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:48:36.625684Z",
     "start_time": "2019-08-30T14:48:36.016933Z"
    }
   },
   "outputs": [],
   "source": [
    "FCluster = tidy_fortracc.resample('min').mean()\n",
    "FCluster = FCluster.dropna()\n",
    "FCluster = FCluster.resample('M')[['N_Cluster']].sum()\n",
    "\n",
    "FCluster2014 = pd.DataFrame({'date':FCluster.index, 'clusters':FCluster.values.ravel()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T14:48:36.771295Z",
     "start_time": "2019-08-30T14:48:36.626930Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=FCluster2014.date, y=FCluster2014['clusters'], name=\"ForTraCC\",\n",
    "                         line_color='deepskyblue'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=cdf2014.date, y=cdf2014['clusters'], name=\"MS 2014\",\n",
    "                         line_color='dimgray'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=cdf2014.date, y=cdf2015['clusters'], name=\"MS 2015\",\n",
    "                         line_color='blueviolet'))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title=go.layout.Title(\n",
    "        text=\"MÃ©dia mensal do nÃºmero de Clusters\",\n",
    "        xref=\"paper\",\n",
    "        xanchor = \"auto\",\n",
    "        x=0,\n",
    "    ),\n",
    "    yaxis=go.layout.YAxis(\n",
    "        title=go.layout.yaxis.Title(\n",
    "            text=\"NÃºmero de Clusters\",\n",
    "            font=dict(\n",
    "                family=\"Courier New, monospace\",\n",
    "                size=14,\n",
    "                color=\"#7f7f7f\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONCLUSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REFERENCES\n",
    "\n",
    "* CHENG, Yizong. Mean shift, mode seeking, and clustering. IEEE transactions on pattern analysis and machine intelligence, v. 17, n. 8, p. 790-799, 1995.\n",
    "\n",
    "* FRANK, Andrew U. Qualitative spatial reasoning: Cardinal directions as an example. International Journal of Geographical Information Science, v. 10, n. 3, p. 269-290, 1996.\n",
    "\n",
    "* Mean Shift Clustering Overview by Matt Nedrich https://spin.atomicobject.com/2015/05/26/mean-shift-clustering/\n",
    "\n",
    "* The NWS NEXRAD WSR88-D Radar: https://web.archive.org/web/20160113151652/http://www.desktopdoppler.com/help/nws-nexrad.htm#rainfall%20rates\n",
    "\n",
    "* PAPADIAS, Dimitris; THEODORIDIS, Yannis. Spatial relations, minimum bounding rectangles, and spatial data structures. International Journal of Geographical Information Science, v. 11, n. 2, p. 111-138, 1997."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
